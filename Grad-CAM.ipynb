{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torchvision\n",
    "import nngen as ng\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=VGG11_Weights.IMAGENET1K_V1`. You can also use `weights=VGG11_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "vgg11 = torchvision.models.vgg11(weights=torchvision.models.VGG11_Weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): ReLU(inplace=True)\n",
       "    (8): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): ReLU(inplace=True)\n",
       "    (10): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (11): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (12): ReLU(inplace=True)\n",
       "    (13): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (14): ReLU(inplace=True)\n",
       "    (15): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (16): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (17): ReLU(inplace=True)\n",
       "    (18): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (19): ReLU(inplace=True)\n",
       "    (20): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vgg11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 手書きGrad-CAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv:\n",
    "    def __init__(self, in_channels, out_channels, params=None, pool=True):\n",
    "        self.conv = torch.nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "        self.pool = pool\n",
    "        self.set(params)\n",
    "    def set(self, params=None):\n",
    "        if params is not None:\n",
    "            self.conv.weight = params.weight\n",
    "            self.conv.bias = params.bias\n",
    "    def forward(self, x):\n",
    "        conved = self.conv(x)\n",
    "        out = torch.relu(conved)\n",
    "        if self.pool:\n",
    "            out = torch.nn.MaxPool2d(kernel_size=2, stride=2, padding=0)(out)\n",
    "        return out\n",
    "\n",
    "class Linear:\n",
    "    def __init__(self, in_features, out_features, params=None, relu=True):\n",
    "        self.linear = torch.nn.Linear(in_features=in_features, out_features=out_features, bias=True)\n",
    "        self.relu = relu\n",
    "        self.set(params)\n",
    "    def set(self, params=None):\n",
    "        if params is not None:\n",
    "            self.linear.weight = params.weight\n",
    "            self.linear.bias = params.bias\n",
    "    def forward(self, x):\n",
    "        out = self.linear(x)\n",
    "        if self.relu:\n",
    "            out = torch.relu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "img_path = \"african_elephant.jpg\"\n",
    "img = Image.open(img_path)\n",
    "img = torchvision.models.VGG11_Weights.IMAGENET1K_V1.transforms()(img)\n",
    "tx = img[None]\n",
    "print(tx.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 512, 7, 7])\n"
     ]
    }
   ],
   "source": [
    "ty1 = Conv(3, 64, vgg11.features[0]).forward(tx)\n",
    "ty2 = Conv(64, 128, vgg11.features[3]).forward(ty1)\n",
    "ty3 = Conv(128, 256, vgg11.features[6], False).forward(ty2)\n",
    "ty4 = Conv(256, 256, vgg11.features[8]).forward(ty3)\n",
    "ty5 = Conv(256, 512, vgg11.features[11], False).forward(ty4)\n",
    "ty6 = Conv(512, 512, vgg11.features[13]).forward(ty5)\n",
    "ty7 = Conv(512, 512, vgg11.features[16], False).forward(ty6)\n",
    "ty = Conv(512, 512, vgg11.features[18]).forward(ty7)\n",
    "print(ty.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1000])\n"
     ]
    }
   ],
   "source": [
    "# z = torch.nn.AvgPool2d(stride=1, kernel_size=1, padding=0)(y)\n",
    "tz1 = torch.flatten(ty, 1)\n",
    "tl2 = Linear(25088, 4096, vgg11.classifier[0])\n",
    "tz2 = tl2.forward(tz1)\n",
    "tz3 = Linear(4096, 4096, vgg11.classifier[3]).forward(tz2)\n",
    "tz = Linear(4096, 1000, vgg11.classifier[6], False).forward(tz3)\n",
    "print(tz.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "386\n"
     ]
    }
   ],
   "source": [
    "torch_res = tz[0]\n",
    "ans = tz.argmax(1).item()\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512, 7, 7)\n"
     ]
    }
   ],
   "source": [
    "ty.retain_grad()\n",
    "tz[0][ans].backward()\n",
    "tdA = ty.grad[0].numpy()\n",
    "print(tdA.shape)\n",
    "tA = ty[0].detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7, 7)\n"
     ]
    }
   ],
   "source": [
    "def relu(x):\n",
    "    return np.where(x > 0, x, np.zeros_like(x))\n",
    "\n",
    "tw = tdA.sum((1, 2)) / 49\n",
    "twA = (np.tile(tw, (7, 7, 1)).transpose((2, 0, 1))) * tA\n",
    "tcam = relu(twA.sum(0))\n",
    "print(tcam.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.colorbar.Colorbar at 0x13fe74ac0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(tcam)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGG11 in NNgen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data types\n",
    "act_dtype = ng.int16\n",
    "weight_dtype = ng.int16\n",
    "bias_dtype = ng.int32\n",
    "scale_dtype = ng.int16\n",
    "sum_dtype = ng.int32\n",
    "batchsize = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv:\n",
    "    def __init__(self, in_channels, out_channels, params=None, pool=True):\n",
    "        self.weight = ng.variable(dtype=weight_dtype, shape=(out_channels, 3, 3, in_channels))\n",
    "        self.bias = ng.variable(dtype=bias_dtype, shape=(self.weight.shape[0],))\n",
    "        self.scale = ng.variable(dtype=scale_dtype, shape=(self.weight.shape[0],))\n",
    "\n",
    "        self.pool = pool\n",
    "        self.set(params)\n",
    "    def set(self, params=None):\n",
    "        self.scale.set_value(np.ones(self.scale.shape))\n",
    "        if params is not None:\n",
    "            self.weight.set_value(params.weight.detach().numpy().transpose(0, 2, 3, 1))\n",
    "            self.bias.set_value(params.bias.detach().numpy())\n",
    "    def forward(self, x):\n",
    "        out = ng.conv2d(x, self.weight,\n",
    "                            strides=(1, 1, 1, 1),\n",
    "                            padding=1,\n",
    "                            bias=self.bias,\n",
    "                            scale=self.scale,\n",
    "                            act_func=ng.relu,\n",
    "                            dtype=act_dtype,\n",
    "                            sum_dtype=sum_dtype)\n",
    "        if self.pool:\n",
    "            pooled = ng.max_pool(out, ksize=(1, 2, 2, 1), strides=(1, 2, 2, 1), padding=0)\n",
    "            return pooled\n",
    "        else:\n",
    "            return out\n",
    "\n",
    "class Linear:\n",
    "    def __init__(self, in_features, out_features, params=None, relu=True):\n",
    "        self.weight = ng.variable(dtype=weight_dtype, shape=(out_features, in_features))\n",
    "        self.bias = ng.variable(dtype=bias_dtype, shape=(self.weight.shape[0],))\n",
    "        self.scale = ng.variable(dtype=scale_dtype, shape=(self.weight.shape[0],))\n",
    "\n",
    "        self.relu = relu\n",
    "        self.set(params)\n",
    "    def set(self, params=None):\n",
    "        self.scale.set_value(np.ones(self.scale.shape))\n",
    "        if params is not None:\n",
    "            self.weight.set_value(params.weight.detach().numpy())\n",
    "            self.bias.set_value(params.bias.detach().numpy())\n",
    "    def set_weight(self, w):\n",
    "        if torch.is_tensor(w):\n",
    "            w = w.detach().numpy()\n",
    "        self.weight.set_value(w)\n",
    "    def set_bias(self, w):\n",
    "        if torch.is_tensor(w):\n",
    "            w = w.detach().numpy()\n",
    "        self.bias.set_value(w)\n",
    "    def forward(self, x):\n",
    "        out = ng.matmul(x, self.weight, bias=self.bias, scale=self.scale, transposed_b=True,\n",
    "                        act_func=ng.relu if self.relu else None,\n",
    "                        dtype=act_dtype, sum_dtype=sum_dtype)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 7, 7, 512)\n"
     ]
    }
   ],
   "source": [
    "nx = ng.placeholder(dtype=act_dtype, shape=(batchsize, 224, 224, 3), name=\"nx\")\n",
    "ny1 = Conv(3, 64, vgg11.features[0]).forward(nx)\n",
    "ny2 = Conv(64, 128, vgg11.features[3]).forward(ny1)\n",
    "ny3 = Conv(128, 256, vgg11.features[6], False).forward(ny2)\n",
    "ny4 = Conv(256, 256, vgg11.features[8]).forward(ny3)\n",
    "ny5 = Conv(256, 512, vgg11.features[11], False).forward(ny4)\n",
    "ny6 = Conv(512, 512, vgg11.features[13]).forward(ny5)\n",
    "ny7 = Conv(512, 512, vgg11.features[16], False).forward(ny6)\n",
    "ny = Conv(512, 512, vgg11.features[18]).forward(ny7)\n",
    "print(ny.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1000)\n"
     ]
    }
   ],
   "source": [
    "nz1 = ng.reshape(ng.transpose(ny, (0, 3, 1, 2)), [batchsize, -1])\n",
    "nl2 = Linear(25088, 4096, vgg11.classifier[0])\n",
    "nz2 = nl2.forward(nz1)\n",
    "nz3 = Linear(4096, 4096, vgg11.classifier[3]).forward(nz2)\n",
    "nz = Linear(4096, 1000, vgg11.classifier[6], False).forward(nz3)\n",
    "print(nz.shape)\n",
    "\n",
    "nzcl = Linear(4096, 1, None, False)\n",
    "nzcl.set_weight(vgg11.classifier[6].weight[ans][None])\n",
    "nzcl.set_bias(vgg11.classifier[6].bias[ans][None])\n",
    "nzc = nzcl.forward(nz3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n"
     ]
    }
   ],
   "source": [
    "imagenet_mean = np.array([0.485, 0.456, 0.406]).astype(np.float32)\n",
    "imagenet_std = np.array([0.229, 0.224, 0.225]).astype(np.float32)\n",
    "\n",
    "if act_dtype.width > 8:\n",
    "    act_scale_factor = 128\n",
    "else:\n",
    "    act_scale_factor = int(round(2 ** (act_dtype.width - 1) * 0.5))\n",
    "print(act_scale_factor)\n",
    "\n",
    "input_scale_factors = {'nx': act_scale_factor}\n",
    "input_means = {'nx': imagenet_mean * act_scale_factor}\n",
    "input_stds = {'nx': imagenet_std * act_scale_factor}\n",
    "\n",
    "ng.quantize([nzc], input_scale_factors, input_means, input_stds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 224, 224, 3)\n",
      "[169 232 291]\n"
     ]
    }
   ],
   "source": [
    "# img_path = \"african_elephant.jpg\"\n",
    "img = Image.open(img_path)\n",
    "img = torchvision.models.VGG11_Weights.IMAGENET1K_V1.transforms()(img)\n",
    "img = img.numpy().transpose((1, 2, 0))[None]\n",
    "print(img.shape)\n",
    "\n",
    "img = np.clip(img, -3.0, 3.0)\n",
    "img = img * act_scale_factor\n",
    "img = np.clip(img, -1 * 2 ** (act_dtype.width - 1) - 1, 2 ** (act_dtype.width - 1))\n",
    "img = np.round(img).astype(np.int64)\n",
    "print(img[0][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[32767]]\n",
      "[-1232  -272 -3330 -4973 -5177]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(ng.eval([nzc], nx=img)[0])\n",
    "\n",
    "gradient = ng.gradient(nzc, nz1)\n",
    "print(gradient[0][:5], end='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7, 7)\n"
     ]
    }
   ],
   "source": [
    "def relu(x):\n",
    "    return np.where(x > 0, x, np.zeros_like(x))\n",
    "\n",
    "ndA = gradient[0].reshape((512, 7, 7))\n",
    "nA = ng.eval([ny], nx=img)[0][0]\n",
    "\n",
    "nw = ndA.sum((1, 2)) / 49\n",
    "nwA = (np.tile(nw, (7, 7, 1))) * nA\n",
    "ncam = relu(nwA.sum(2))\n",
    "print(ncam.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.colorbar.Colorbar at 0x13ffbbe20>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(ncam)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.999870427084722"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def corrcoef(x, y):\n",
    "    if torch.is_tensor(x):\n",
    "        x = x.detach().numpy()\n",
    "        if len(x.shape) == 4:\n",
    "            x = x.transpose(0, 2, 3, 1)\n",
    "    if torch.is_tensor(y):\n",
    "        y = y.detach().numpy()\n",
    "        if len(y.shape) == 4:\n",
    "            y = y.transpose(0, 2, 3, 1)\n",
    "    return np.corrcoef(x.flatten(), y.flatten())[0][1]\n",
    "\n",
    "corrcoef(tcam, ncam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6ecdebf77f2ee3a47348d003f751c63e810ca996c1c68d1179f338200fa83b34"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
