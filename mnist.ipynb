{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3.40\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "# from sklearn.datasets import fetch_mldata\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy as np\n",
    "import nngen as ng\n",
    "from pathlib import Path\n",
    "print(ng.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return np.tanh(x * 0.5) * 0.5 + 0.5\n",
    "\n",
    "def deriv_sigmoid(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(x, 0)\n",
    "\n",
    "def deriv_relu(x):\n",
    "    return (x > 0).astype(x.dtype)\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def deriv_tanh(x):\n",
    "    return 1 - np.tanh(x)**2\n",
    "\n",
    "# logの中身が 0になるのを防ぐ\n",
    "def np_log(x):\n",
    "    return np.log(np.clip(x, 1e-8, None))\n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    x -= x.max(axis=1, keepdims=True)\n",
    "    x_exp = np.exp(x)\n",
    "    return x_exp / np.sum(x_exp, axis=1, keepdims=True)\n",
    "\n",
    "def deriv_softmax(x):\n",
    "    return softmax(x) * (1 - softmax(x))\n",
    "\n",
    "def identity(x):\n",
    "    return x[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sefutsu/.local/lib/python3.10/site-packages/sklearn/datasets/_openml.py:932: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70000, 784)\n",
      "(10000, 784)\n"
     ]
    }
   ],
   "source": [
    "# mnist = fetch_mldata('MNIST original')\n",
    "mnist = fetch_openml('mnist_784')\n",
    "\n",
    "x_mnist = mnist.data.to_numpy().astype('float32') / 255.\n",
    "t_mnist = np.eye(10)[mnist.target.to_numpy().astype('int32')]\n",
    "print(x_mnist.shape)\n",
    "\n",
    "x_train_mnist, x_test_mnist, t_train_mnist, t_test_mnist = train_test_split(x_mnist, t_mnist, test_size=50000)\n",
    "x_train_mnist, x_valid_mnist, t_train_mnist, t_valid_mnist = train_test_split(x_train_mnist, t_train_mnist, test_size=10000)\n",
    "print(x_train_mnist.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data types\n",
    "act_dtype = ng.int8\n",
    "weight_dtype = ng.int8\n",
    "bias_dtype = ng.int32\n",
    "scale_dtype = ng.int8\n",
    "batchsize = 1\n",
    "\n",
    "if act_dtype.width > 8:\n",
    "    act_scale_factor = 128\n",
    "else:\n",
    "    act_scale_factor = int(round(2 ** (act_dtype.width - 1) * 0.5))\n",
    "input_scale_factors = {'l0': act_scale_factor}\n",
    "input_means = {'l0': x_train_mnist.mean() * act_scale_factor}\n",
    "input_stds = {'l0': x_train_mnist.std() * act_scale_factor}\n",
    "\n",
    "class Matmul:\n",
    "    def __init__(self, input_node, in_dim, out_dim, name, act=True):\n",
    "        self.name = name\n",
    "        self.W = np.random.uniform(low=-0.08, high=0.08,\n",
    "                                   size=(in_dim, out_dim)).astype('float32')\n",
    "        self.b = np.zeros(out_dim).astype('float32')\n",
    "        if act:\n",
    "            self.act = relu\n",
    "            self.deriv_act = deriv_relu\n",
    "        else:\n",
    "            self.act = identity\n",
    "            self.deriv_act = identity\n",
    "        \n",
    "        self.x = None\n",
    "        self.u = None\n",
    "        \n",
    "        self.dW = 0\n",
    "        self.db = 0\n",
    "\n",
    "        self.input_node = input_node\n",
    "        \n",
    "        self.W_ng = ng.variable(dtype=weight_dtype, shape=(out_dim, in_dim))\n",
    "        self.b_ng = ng.variable(dtype=bias_dtype, shape=(out_dim))\n",
    "        self.s_ng = ng.variable(dtype=scale_dtype, shape=(out_dim))\n",
    "        self.out_ng = ng.matmul(self.input_node.out_ng, self.W_ng,\n",
    "            bias=self.b_ng, scale=self.s_ng, transposed_b=True,\n",
    "            act_func=ng.relu if act else None, dtype=act_dtype, sum_dtype=bias_dtype)\n",
    "\n",
    "    def reset_ng(self):\n",
    "        out_dim, in_dim = self.W_ng.shape\n",
    "        self.W_ng.__init__(dtype=weight_dtype, shape=(out_dim, in_dim))\n",
    "        self.b_ng.__init__(dtype=bias_dtype, shape=(out_dim))\n",
    "        self.s_ng.__init__(dtype=scale_dtype, shape=(out_dim))\n",
    "        self.out_ng.__init__(self.input_node.out_ng, self.W_ng,\n",
    "            bias=self.b_ng, scale=self.s_ng, transposed_b=True,\n",
    "            act_func=ng.relu if self.out_ng.act_func else None, dtype=act_dtype, sum_dtype=bias_dtype)\n",
    "\n",
    "    def forward_np(self, feed_dict):\n",
    "        self.x = self.input_node.forward_np(feed_dict)\n",
    "        self.u = np.matmul(self.x, self.W) + self.b\n",
    "        return self.act(self.u)\n",
    "\n",
    "    def backward_np(self, delta, W=None):\n",
    "        if W is None: #出力層\n",
    "            self.delta = delta\n",
    "        else:\n",
    "            self.delta = self.deriv_act(self.u) * np.matmul(delta, W.T)\n",
    "        self.compute_grad()\n",
    "        self.input_node.backward_np(self.delta, self.W)\n",
    "    \n",
    "    def compute_grad(self):\n",
    "        batch_size = self.delta.shape[0]\n",
    "        self.dW += np.matmul(self.x.T, self.delta) / batch_size\n",
    "        self.db += np.matmul(np.ones(batch_size), self.delta) / batch_size\n",
    "\n",
    "    def update_params(self, alpha):\n",
    "        self.W -= alpha * self.dW\n",
    "        self.b -= alpha * self.db\n",
    "        self.dW = 0\n",
    "        self.db = 0\n",
    "        self.input_node.update_params(alpha)\n",
    "        \n",
    "    def sync_params(self):\n",
    "        self.input_node.sync_params()\n",
    "        self.reset_ng()\n",
    "        self.W_ng.set_value(self.W.T)\n",
    "        self.b_ng.set_value(self.b)\n",
    "        self.s_ng.set_value(np.ones(self.s_ng.shape))\n",
    "        \n",
    "    def forward_ng(self, feed_dict):\n",
    "        self.x = self.input_node.forward_ng(feed_dict)\n",
    "        # 本当は間違いだが、backwardのderiv_reluに渡すためだけなら大丈夫\n",
    "        self.u = ng.eval([self.out_ng], **feed_dict)[0].astype(float) / self.out_ng.scale_factor\n",
    "        return self.u\n",
    "    \n",
    "    def backward_ng(self, delta, W=None):\n",
    "        if W is None: #出力層\n",
    "            self.delta = delta\n",
    "        else:\n",
    "            self.delta = self.deriv_act(self.u) * np.matmul(delta, W.T)\n",
    "        self.compute_grad()\n",
    "        self.input_node.backward_ng(self.delta, self.W)\n",
    "    \n",
    "    def save_params_np(self, path):\n",
    "        self.input_node.save_params_np(path)\n",
    "        np.save(Path(path) / (self.name + \"_w.npy\"), self.W)\n",
    "        np.save(Path(path) / (self.name + \"_b.npy\"), self.b)\n",
    "\n",
    "    def load_params_np(self, path):\n",
    "        self.input_node.load_params_np(path)\n",
    "        self.W = np.load(Path(path) / (self.name + \"_w.npy\"))\n",
    "        self.b = np.load(Path(path) / (self.name + \"_b.npy\"))\n",
    "\n",
    "class PlaceHolder:\n",
    "    def __init__(self, ch, name):\n",
    "        self.name = name\n",
    "        self.out_ng = ng.placeholder(dtype=act_dtype, shape=(batchsize, ch), name=name)\n",
    "    def forward_np(self, feed_dict):\n",
    "        return feed_dict[self.name]\n",
    "    def backward_np(self, delta, W):\n",
    "        pass\n",
    "    def update_params(self, alpha):\n",
    "        pass\n",
    "    def sync_params(self):\n",
    "        pass\n",
    "    def forward_ng(self, feed_dict):\n",
    "        return feed_dict[self.name] / act_scale_factor\n",
    "    def backward_ng(self, delta, W):\n",
    "        pass\n",
    "    def save_params_np(self, path):\n",
    "        pass\n",
    "    def load_params_np(self, path):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "l0 = PlaceHolder(784, \"l0\")\n",
    "l1 = Matmul(l0, 784, 100, \"l1\")\n",
    "l2 = Matmul(l1, 100, 100, \"l2\")\n",
    "l3 = Matmul(l2, 100, 10, \"l3\", False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NNgen: Neural Network Accelerator Generator (version 1.3.40)\n",
      "[IP-XACT]\n",
      "  Output: mlp\n",
      "[Configuration]\n",
      "(AXI Master Interface)\n",
      "  Data width   : 32\n",
      "  Address width: 32\n",
      "(AXI Slave Interface)\n",
      "  Data width   : 32\n",
      "  Address width: 32\n",
      "[Schedule Table]\n",
      "(Stage 0)\n",
      "(Stage 1)\n",
      "  <matmul None dtype:int8 shape:(1, 100) bias:(100,) scale:(100,) act_func:relu sum_dtype:int32 par_left_col:2 par_out_col:2 concur_out_col:4 stationary:right keep_left default_addr:91648 g_index:0 l_index:1 word_alignment:4 aligned_shape:(1, 100) scale_factor:1.000000>\n",
      "  | <placeholder l0 dtype:int8 shape:(1, 784) default_addr:64 g_index:2 word_alignment:4 aligned_shape:(1, 784) scale_factor:1.000000>\n",
      "  | <variable input_0 dtype:int8 shape:(100, 784) default_addr:896 g_index:3 word_alignment:4 aligned_shape:(100, 784) scale_factor:1.000000>\n",
      "  | <variable input_1 dtype:int32 shape:(100,) default_addr:896 g_index:3 word_alignment:2 aligned_shape:(100,) scale_factor:1.000000>\n",
      "  | <variable input_2 dtype:int8 shape:(100,) default_addr:896 g_index:3 word_alignment:4 aligned_shape:(100,) scale_factor:1.000000>\n",
      "(Stage 2)\n",
      "  <matmul None dtype:int8 shape:(1, 100) bias:(100,) scale:(100,) act_func:relu sum_dtype:int32 par_left_col:2 par_out_col:2 concur_out_col:40 stationary:right keep_left default_addr:91776 g_index:0 l_index:2 word_alignment:4 aligned_shape:(1, 100) scale_factor:1.000000>\n",
      "  | <matmul None dtype:int8 shape:(1, 100) bias:(100,) scale:(100,) act_func:relu sum_dtype:int32 par_left_col:2 par_out_col:2 concur_out_col:4 stationary:right keep_left default_addr:91648 g_index:0 l_index:1 word_alignment:4 aligned_shape:(1, 100) scale_factor:1.000000>\n",
      "  | <variable input_3 dtype:int8 shape:(100, 100) default_addr:896 g_index:3 word_alignment:4 aligned_shape:(100, 100) scale_factor:1.000000>\n",
      "  | <variable input_4 dtype:int32 shape:(100,) default_addr:896 g_index:3 word_alignment:2 aligned_shape:(100,) scale_factor:1.000000>\n",
      "  | <variable input_5 dtype:int8 shape:(100,) default_addr:896 g_index:3 word_alignment:4 aligned_shape:(100,) scale_factor:1.000000>\n",
      "(Stage 3)\n",
      "  <matmul output_matmul_0 dtype:int8 shape:(1, 10) bias:(10,) scale:(10,) sum_dtype:int32 par_left_col:2 par_out_col:2 concur_out_col:40 stationary:right keep_left keep_right default_addr:0 g_index:1 word_alignment:4 aligned_shape:(1, 12) scale_factor:1.000000>\n",
      "  | <matmul None dtype:int8 shape:(1, 100) bias:(100,) scale:(100,) act_func:relu sum_dtype:int32 par_left_col:2 par_out_col:2 concur_out_col:40 stationary:right keep_left default_addr:91776 g_index:0 l_index:2 word_alignment:4 aligned_shape:(1, 100) scale_factor:1.000000>\n",
      "  | <variable input_6 dtype:int8 shape:(10, 100) default_addr:896 g_index:3 word_alignment:4 aligned_shape:(10, 100) scale_factor:1.000000>\n",
      "  | <variable input_7 dtype:int32 shape:(10,) default_addr:896 g_index:3 word_alignment:2 aligned_shape:(10,) scale_factor:1.000000>\n",
      "  | <variable input_8 dtype:int8 shape:(10,) default_addr:896 g_index:3 word_alignment:4 aligned_shape:(12,) scale_factor:1.000000>\n",
      "[RAM (spec: num)]\n",
      "  64-bit 128-entry 2-port 1-bank RAM: 1\n",
      "  16-bit 2048-entry 2-port 2-bank RAM: 2\n",
      "  16-bit 512-entry 2-port 2-bank RAM: 3\n",
      "[Substream (spec: num)]\n",
      "  ('acc_rshift_round_frac', (32, 0, True, 32, 0, True)): 2\n",
      "  ('add_tree', (32, 0, True, 2)): 2\n",
      "  ('mul_rshift_round_clip', (32, 0, True, 8, 0, True, 40, 0, True, 8, 0, True, False)): 2\n",
      "  ('mul_rshift_round_madd', (8, 0, True, 8, 0, True, 16, 0, True)): 4\n",
      "[Stream (spec: num)]\n",
      "  (((<class 'nngen.operator.matmul.matmul'>, <dtype int8>, <dtype int8>, <dtype int32>, <dtype int8>), <dtype int8>, 1), 1, 1, False, None, <dtype int32>, 2, 2, 1, 1, 1, 4): 1\n",
      "[State IDs in main_fsm]\n",
      "  (3, 4, 'l0', 'None')\n",
      "  (12, 14, None, 'control_matmul_4')\n",
      "  (22, 24, None, 'control_matmul_4')\n",
      "  (32, 34, 'output_matmul_0', 'control_matmul_4')\n",
      "[Control (name (# states: num))]\n",
      "  main_fsm (# states: 40)\n",
      "  control_matmul_4 (# states: 29)\n",
      "[Register Map]\n",
      "    0 (R ): header0 (default: 0x00000000)\n",
      "    4 (R ): header1 (default: 0x00000000)\n",
      "    8 (R ): header2 (default: 0x00000000)\n",
      "   12 (R ): header3 (default: 0x00000000)\n",
      "   16 ( W): Start (set '1' to run)\n",
      "   20 (R ): Busy (returns '1' when running)\n",
      "   24 ( W): Reset (set '1' to initialize internal logic)\n",
      "   28 (R ): Opcode from extern objects to SW (returns '0' when idle)\n",
      "   32 ( W): Resume extern objects (set '1' to resume)\n",
      "   36 (R ): Interrupt Status Register\n",
      "   40 ( W): Interrupt Enable Register\n",
      "   44 ( W): Interrupt Acknowledge Register\n",
      "   48 (R ): State Counter\n",
      "   52 ( W): Count Target\n",
      "   56 ( W): Count Divider\n",
      "   60 (  ): Reserved ...\n",
      "  120 (  ): ... Reserved\n",
      "  124 (R ): Address space amount\n",
      "  128 (RW): Global address offset (default: 0)\n",
      "  132 (RW): Address of temporal storage (matmul) output_matmul_0.None (size: 128B)\n",
      "  136 (RW): Address of output (matmul) 'output_matmul_0' (size: 64B, dtype: int8, shape: (1, 10), alignment: 4 words (4 bytes)), aligned shape: (1, 12)\n",
      "  140 (RW): Address of placeholder 'l0' (size: 832B, dtype: int8, shape: (1, 784), alignment: 4 words (4 bytes)), aligned shape: (1, 784)\n",
      "  144 (RW): Address of variables 'input_0', 'input_1', 'input_2', 'input_3', 'input_4', 'input_5', 'input_6', 'input_7', 'input_8' (size: 89KB)\n",
      "[Default Memory Map (start - end)] (entire range: [0 - 91903], size: 90KB)\n",
      "  [    0 -    63]: output (matmul) 'output_matmul_0' (size: 64B, dtype: int8, shape: (1, 10), alignment: 4 words (4 bytes)), aligned shape: (1, 12)\n",
      "  [   64 -   895]: placeholder 'l0' (size: 832B, dtype: int8, shape: (1, 784), alignment: 4 words (4 bytes)), aligned shape: (1, 784)\n",
      "  [  896 - 79295]: variable 'input_0' (size: 77KB, dtype: int8, shape: (100, 784), alignment: 4 words (4 bytes)), aligned shape: (100, 784)\n",
      "  [79296 - 79743]: variable 'input_1' (size: 448B, dtype: int32, shape: (100,), alignment: 2 words (8 bytes)), aligned shape: (100,)\n",
      "  [79744 - 79871]: variable 'input_2' (size: 128B, dtype: int8, shape: (100,), alignment: 4 words (4 bytes)), aligned shape: (100,)\n",
      "  [79872 - 89919]: variable 'input_3' (size: 10KB, dtype: int8, shape: (100, 100), alignment: 4 words (4 bytes)), aligned shape: (100, 100)\n",
      "  [89920 - 90367]: variable 'input_4' (size: 448B, dtype: int32, shape: (100,), alignment: 2 words (8 bytes)), aligned shape: (100,)\n",
      "  [90368 - 90495]: variable 'input_5' (size: 128B, dtype: int8, shape: (100,), alignment: 4 words (4 bytes)), aligned shape: (100,)\n",
      "  [90496 - 91519]: variable 'input_6' (size: 1KB, dtype: int8, shape: (10, 100), alignment: 4 words (4 bytes)), aligned shape: (10, 100)\n",
      "  [91520 - 91583]: variable 'input_7' (size: 64B, dtype: int32, shape: (10,), alignment: 2 words (8 bytes)), aligned shape: (10,)\n",
      "  [91584 - 91647]: variable 'input_8' (size: 64B, dtype: int8, shape: (10,), alignment: 4 words (4 bytes)), aligned shape: (12,)\n",
      "  [91648 - 91775]: temporal storage (matmul) None.None (size: 128B)\n",
      "  [91776 - 91903]: temporal storage (matmul) output_matmul_0.None (size: 128B)\n"
     ]
    }
   ],
   "source": [
    "# --------------------\n",
    "# (3) Assign hardware attributes\n",
    "# --------------------\n",
    "\n",
    "par_ich = 2\n",
    "par_och = 2\n",
    "axi_datawidth = 32\n",
    "\n",
    "l1.out_ng.attribute(par_ich=par_ich, par_och=par_och)\n",
    "l2.out_ng.attribute(par_ich=par_ich, par_och=par_och)\n",
    "l3.out_ng.attribute(par_ich=par_ich, par_och=par_och)\n",
    "\n",
    "# --------------------\n",
    "# (5) Convert the NNgen dataflow to a hardware description (Verilog HDL and IP-XACT)\n",
    "# --------------------\n",
    "\n",
    "# to Veriloggen object\n",
    "# targ = ng.to_veriloggen([output_layer], 'mlp', silent=silent,\n",
    "#                        config={'maxi_datawidth': axi_datawidth})\n",
    "\n",
    "# to IP-XACT (the method returns Veriloggen object, as well as to_veriloggen)\n",
    "targ = ng.to_ipxact([l3.out_ng], 'mlp',\n",
    "                    config={'maxi_datawidth': axi_datawidth})\n",
    "\n",
    "# to Verilog HDL RTL (the method returns a source code text)\n",
    "# rtl = ng.to_verilog([output_layer], 'mlp', silent=silent,\n",
    "#                    config={'maxi_datawidth': axi_datawidth})\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "5a1a0aa0c91ce0ab7ac00e66a3070a355af318d9538f0420f8c438269f048218"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
